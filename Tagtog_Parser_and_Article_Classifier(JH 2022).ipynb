{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exposed-approach",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JonH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import fnmatch\n",
    "import getpass\n",
    "import glob\n",
    "#!pip install html5lib\n",
    "import html5lib\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "from argparse import RawTextHelpFormatter\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import islice, chain\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.metrics.scores import precision, recall\n",
    "from nltk import word_tokenize, bigrams\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for processing human trafficking news article annotations from \n",
    "# https://www.tagtog.com/jhudlow/HT_News/pool. \n",
    "#Full project guidelines can be found here: https://www.tagtog.com/jhudlow/HT_News/-settings. \n",
    "\n",
    "class Annotation(object):\n",
    "    \"\"\"This is a class for annotations by individuals or 'master' versions.\"\"\"\n",
    "    def __init__(self, relevant, curator):\n",
    "        self.relevant = relevant\n",
    "        self.curator = curator\n",
    "\n",
    "        \n",
    "class Article(object):\n",
    "    \"\"\"This is a class for news articles.\"\"\"\n",
    "    objects = []\n",
    "    \n",
    "    def __init__(self, uid, url, text):\n",
    "        self.uid = uid\n",
    "        self.url = url\n",
    "        self.text = text\n",
    "        self.ann1 = None\n",
    "        self.ann2 = None\n",
    "        self.master_ann = None\n",
    "        self.__class__.objects.append(self)\n",
    "        \n",
    "    @classmethod\n",
    "    def merge_to_master(cls, print_con_url=False):\n",
    "        \"\"\"Update 'master' versions of 'relevant' to the values entered by ann1 and ann2 (first and \n",
    "        second annotations) when they are in agreement. When they are not, update to 'disagree'. \n",
    "        Optionally print article url for conflicting annotations.\"\"\"\n",
    "        twice_ann = 0 # number annotated twice\n",
    "        once_ann = 0 # number annotated once\n",
    "        con_ann = 0 # number of conflicting annotations\n",
    "        for obj in cls.objects:\n",
    "            if obj.ann2 != None and obj.ann1 != None:\n",
    "                twice_ann += 1\n",
    "                if obj.master_ann == None:\n",
    "                    try:\n",
    "                        if obj.ann1.relevant == obj.ann2.relevant:\n",
    "                            master_relevant = obj.ann2.relevant\n",
    "                        else:\n",
    "                            master_relevant = 'disagree'\n",
    "                            con_ann += 1\n",
    "                            if print_con_url==True:\n",
    "                                print(obj.url)\n",
    "                        obj.master_ann = Annotation(master_relevant, 'master')\n",
    "                    except:\n",
    "                        print('Error getting annotations for {}'.format(str(obj.uid)))\n",
    "            elif obj.ann1 != None or obj.ann2 != None:\n",
    "                once_ann += 1\n",
    "        print(\"Number of articles annotated only once: {} \\n\".format(once_ann) + \\\n",
    "             \"Number of articles annotated twice: {} \\n\".format(twice_ann) + \\\n",
    "             \"Number of remaining conflicts between 1st and 2nd annotations: {}\".format(con_ann))\n",
    "    \n",
    "    @classmethod\n",
    "    def get_article_relevance_conflicts(cls):\n",
    "        num_rel_conflicts = 0\n",
    "        for obj in cls.objects:\n",
    "            try:\n",
    "                if obj.master_ann.relevant =='disagree':\n",
    "                    num_rel_conflicts += 1\n",
    "                    if obj.ann1.relevant=='old' or obj.ann2.relevan=='old':\n",
    "                        \n",
    "                        print(\"Rel \" + str(num_rel_conflicts) \\\n",
    "                              + \": https://tagtog.net/jhudlow/HT_News/-search/members_anncomplete:\" \\\n",
    "                              + obj.ann2.curator + \"/\" + obj.uid)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Number of relevant conflicts:\" + str(num_rel_conflicts))\n",
    "        \n",
    "    @classmethod\n",
    "    def get_confirmed_rel(cls):\n",
    "        num_confirmed_rel = 0\n",
    "        confirmed_rel = []\n",
    "        for obj in cls.objects:\n",
    "            try:\n",
    "                if obj.master_ann.relevant !='disagree':\n",
    "                    num_confirmed_rel += 1\n",
    "                    confirmed_rel.append(obj.uid)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Number of confirmed relevant articles:\" + str(num_confirmed_rel))\n",
    "        return confirmed_rel\n",
    "    \n",
    "    @classmethod\n",
    "    def get_rel_confirmed(cls):\n",
    "        num_rel_confirmed = 0\n",
    "        rel_confirmed = []\n",
    "        for obj in cls.objects:\n",
    "            try:\n",
    "                if obj.master_ann.relevant != 'disagree':\n",
    "                    num_rel_confirmed += 1\n",
    "                    rel_confirmed.append(obj.uid)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Number of confirmed relevant articles: \" + str(num_rel_confirmed))\n",
    "        return rel_confirmed\n",
    "    \n",
    "    @classmethod\n",
    "    def get_total_rel_cat(cls, cat, include):\n",
    "        \"\"\"Gets a list of all article uids for a given relevance category which are labeled \n",
    "        as such by either or both annotations.\n",
    "        \n",
    "        Args:\n",
    "        cat: string corresponding to the relevance category.\n",
    "        include: 'both' - returns articles where both annotators are in agreement about this category.\n",
    "            'either' - returns articles where one or both annotators are in agreement.\n",
    "        \"\"\"\n",
    "        rel_cat_num = 0\n",
    "        total_rel_cat = []\n",
    "        for obj in cls.objects:\n",
    "            try:\n",
    "                if include == 'both':\n",
    "                    if obj.master_ann.relevant == cat:\n",
    "                        rel_cat_num += 1\n",
    "                        total_rel_cat.append(obj.uid)\n",
    "                elif include == 'either':\n",
    "                    if (obj.ann1.relevant == cat) | (obj.ann2.relevant == cat):\n",
    "                        rel_cat_num += 1\n",
    "                        total_rel_cat.append(obj.uid)\n",
    "                else:\n",
    "                    print('incorrect argument for \"include\" parameter')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        print(\"Number of {} articles:\".format(cat) + str(rel_cat_num))\n",
    "        return total_rel_cat\n",
    "    \n",
    "\n",
    "def get_article_dict(path_to_tagtog_folder):\n",
    "    \"\"\"Create 'Article' class instances for all articles that have html files and return them\n",
    "    in a dictonary where each key is the article's uid.\"\"\"\n",
    "    \n",
    "    path_to_html = path_to_tagtog_folder + r\"\\HT_News\\plain.html\\pool\"\n",
    "    html_files = [pos_html for pos_html in os.listdir(path_to_html) if pos_html.endswith('.html')]\n",
    "    \n",
    "    a_dict = {}\n",
    "\n",
    "    for i in range(len(html_files)):\n",
    "        f = open(path_to_html + str(\"\\\\\") + html_files[i], encoding=\"utf8\")\n",
    "        parsed_html = BeautifulSoup(f)\n",
    "        \n",
    "        # Get URL\n",
    "        if \"http\" in parsed_html.body.find('pre', attrs={'id':\"s1s2v1\"}).text:\n",
    "            url = parsed_html.body.find('pre', attrs={'id':\"s1s2v1\"}).text\n",
    "        elif \"http\" in parsed_html.body.find('pre', attrs={'id':\"s1s5v1\"}).text:\n",
    "            url = parsed_html.body.find('pre', attrs={'id':\"s1s5v1\"}).text\n",
    "        else:\n",
    "            print(\"Error: URL not found for file {}\".format(html_files[i]))\n",
    "\n",
    "        # Get Article Text\n",
    "        if str(parsed_html(text=re.compile(r'article'))[0].parent)[1:3]== 'h2':\n",
    "            str_id = str(parsed_html(text=re.compile(r'article'))[0].parent)[8:12]+str('v1')\n",
    "        elif str(parsed_html(text=re.compile(r'article'))[1].parent)[1:3]== 'h2':\n",
    "            str_id = str(parsed_html(text=re.compile(r'article'))[1].parent)[8:12]+str('v1')\n",
    "        else:\n",
    "            print(\"Error: Text not found for file {}\".format(html_files[i]))\n",
    "        article_text = str(parsed_html.body.find(\n",
    "            'pre', attrs={'id':str_id}))[17:].replace('\\n', ' ').replace('</pre>', '')\n",
    "    \n",
    "        a_dict[html_files[i][:-11]] = Article(html_files[i][:-11], url, article_text)\n",
    "    \n",
    "    return a_dict\n",
    "\n",
    "\n",
    "def update_doc_master_anns(path_to_tagtog_folder, a_dict):\n",
    "    \"\"\"Get the latest document level annotations from all members and add them to Article class\n",
    "    instances in a_dict.\"\"\"\n",
    "    path_to_masters = path_to_tagtog_folder + r\"\\HT_News\\ann.json\\master\\pool\"\n",
    "    json_master_files = [pos_json for pos_json in os.listdir(path_to_masters) if pos_json.endswith('.json')]\n",
    "    for j in range(len(json_master_files)):\n",
    "        f = open(path_to_masters + str(\"\\\\\") + json_master_files[j])\n",
    "        data = json.load(f)\n",
    "        try:\n",
    "            relevant = data.get('metas')['m_10']['value']\n",
    "        except:\n",
    "            relevant = 'incomplete'\n",
    "        uid = json_master_files[j][:-9]\n",
    "        if a_dict[uid].master_ann == None:\n",
    "            a_dict[uid].master_ann = Annotation(relevant, 'master')\n",
    "        else:\n",
    "            print('Already have master ann for {}'.format(uid))\n",
    "\n",
    "                \n",
    "def update_doc_annotations(path_to_tagtog_folder, a_dict, print_incomplete_ann=True):\n",
    "    \"\"\"Get the latest document level annotations from all members and add them to Article class\n",
    "    instances in a_dict.\"\"\"\n",
    "    path_to_mem_folders = path_to_tagtog_folder + r\"\\HT_News\\ann.json\\members\"\n",
    "    mem_folders = os.listdir(path_to_mem_folders)\n",
    "    \n",
    "    json_files = []\n",
    "    for i in range(len(mem_folders)):\n",
    "        path_to_json = path_to_tagtog_folder + r\"\\HT_News\\ann.json\\members\" + str(\"\\\\\") \\\n",
    "        + mem_folders[i] + str(\"\\\\pool\")\n",
    "        json_mem_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "        for j in range(len(json_mem_files)):\n",
    "            f = open(path_to_json + str(\"\\\\\") + json_mem_files[j])\n",
    "            data = json.load(f)\n",
    "            if data.get('anncomplete') == True:\n",
    "                if \"m_12\" not in data.get('metas'):\n",
    "                    try:\n",
    "                        relevant = data.get('metas')['m_10']['value']\n",
    "                        curator = str(data.get('metas')['m_10']['confidence']['who'])[7:-2]\n",
    "                    except:\n",
    "                        relevant = 'incomplete'\n",
    "                else:\n",
    "                    relevant = 'old'\n",
    "                    curator = 'n/a'\n",
    "                uid = json_mem_files[j][:-9]\n",
    "                if a_dict[uid].ann1 == None:\n",
    "                    a_dict[uid].ann1 = Annotation(relevant, curator)\n",
    "                else:\n",
    "                    a_dict[uid].ann2 = Annotation(relevant, curator)\n",
    "            else:\n",
    "                if print_incomplete_ann:\n",
    "                    print('ann incomplete for {}: {}'.format(mem_folders[i], json_mem_files[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "olive-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading article annotations from Tagtog, extract relevant info:\n",
    "\n",
    "tagtog_path = os.getcwd() + \"\\\\tagtog_HT_News_4_1_22\"\n",
    "a_dict = get_article_dict(tagtog_path)\n",
    "update_doc_annotations(tagtog_path, a_dict)\n",
    "update_doc_master_anns(tagtog_path, a_dict)\n",
    "#Article.merge_to_master()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ordinary-permission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "op_relevant: 57, not_relevant: 49\n"
     ]
    }
   ],
   "source": [
    "rel_confirmed = list(a_dict.keys())\n",
    "#Article.get_rel_confirmed()\n",
    "op_relevant = []\n",
    "not_relevant = []\n",
    "rel_dict = {}\n",
    "\n",
    "for i in range(len(list(a_dict.keys()))):\n",
    "    try:\n",
    "        if a_dict[rel_confirmed[i]].master_ann.relevant=='op_relevant':\n",
    "            rel_dict[rel_confirmed[i]] = 'op_relevant'\n",
    "            op_relevant.append(rel_confirmed[i])\n",
    "        elif a_dict[rel_confirmed[i]].master_ann.relevant=='not_relevant':\n",
    "            rel_dict[rel_confirmed[i]] = 'not_relevant'\n",
    "            not_relevant.append(rel_confirmed[i])\n",
    "    except:\n",
    "        pass\n",
    "print(\"\\nop_relevant: {}, not_relevant: {}\".format(str(len(op_relevant)), str(len(not_relevant))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "involved-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = [([i], 'not_relevant') for i in not_relevant] + [([i], 'op_relevant') for i in op_relevant]\n",
    "train, test = train_test_split(train_test, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complicated-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View text of a particular article:\n",
    "#a_dict[train[2][0][0]].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "graduate-given",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featuresets: 106\n"
     ]
    }
   ],
   "source": [
    "corp = []\n",
    "for i in range(len(train_test)):\n",
    "    corp += word_tokenize(re.sub(r'\\d+', '', a_dict[train_test[i][0][0]].text))\n",
    "\n",
    "all_words = nltk.FreqDist(corp)\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = [',', \"'\", 'of', 'the', '.', \"'\\\\r\\\\n\", 'and', 'in', 'a']\n",
    "\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "new_words = [word for word in all_words if word.isalnum()]\n",
    "filtered_words = [w for w in new_words if not w.lower() in stopWords]\n",
    "filtered_words = nltk.FreqDist(filtered_words)\n",
    "len(filtered_words)\n",
    "\n",
    "word_features = list(filtered_words)[:1000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(word_tokenize(re.sub(r'\\d+', '', a_dict[arti[0]].text))), category) for (arti, category) in train_test]\n",
    "print('Featuresets: {}'.format(str(len(featuresets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "graphic-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent: 69.76744186046511\n",
      "Most Informative Features\n",
      "                   woman = True           op_rel : not_re =      7.3 : 1.0\n",
      "                   given = True           not_re : op_rel =      6.6 : 1.0\n",
      "                     job = True           op_rel : not_re =      6.3 : 1.0\n",
      "                   media = True           op_rel : not_re =      6.3 : 1.0\n",
      "                 calling = True           not_re : op_rel =      5.7 : 1.0\n",
      "              department = True           not_re : op_rel =      5.7 : 1.0\n",
      "                 include = True           not_re : op_rel =      5.7 : 1.0\n",
      "                  within = True           not_re : op_rel =      5.7 : 1.0\n",
      "                  arrest = True           op_rel : not_re =      5.3 : 1.0\n",
      "                    deal = True           not_re : op_rel =      4.8 : 1.0\n",
      "               developed = True           not_re : op_rel =      4.8 : 1.0\n",
      "                  impact = True           not_re : op_rel =      4.8 : 1.0\n",
      "                increase = True           not_re : op_rel =      4.8 : 1.0\n",
      "                    next = True           not_re : op_rel =      4.8 : 1.0\n",
      "                   share = True           not_re : op_rel =      4.8 : 1.0\n",
      "                  matter = True           op_rel : not_re =      4.8 : 1.0\n",
      "                  mother = True           op_rel : not_re =      4.8 : 1.0\n",
      "                  others = True           op_rel : not_re =      4.8 : 1.0\n",
      "                    sold = True           op_rel : not_re =      4.4 : 1.0\n",
      "                 accused = True           op_rel : not_re =      4.3 : 1.0\n",
      "                 claimed = True           op_rel : not_re =      4.3 : 1.0\n",
      "                incident = True           op_rel : not_re =      4.3 : 1.0\n",
      "                   night = True           op_rel : not_re =      4.3 : 1.0\n",
      "                     New = True           not_re : op_rel =      4.0 : 1.0\n",
      "               September = True           not_re : op_rel =      4.0 : 1.0\n",
      "                      UN = True           not_re : op_rel =      4.0 : 1.0\n",
      "              University = True           not_re : op_rel =      4.0 : 1.0\n",
      "                  anyone = True           not_re : op_rel =      4.0 : 1.0\n",
      "                   aware = True           not_re : op_rel =      4.0 : 1.0\n",
      "               awareness = True           not_re : op_rel =      4.0 : 1.0\n",
      "                   clear = True           not_re : op_rel =      4.0 : 1.0\n",
      "                included = True           not_re : op_rel =      4.0 : 1.0\n",
      "             significant = True           not_re : op_rel =      4.0 : 1.0\n",
      "               something = True           not_re : op_rel =      4.0 : 1.0\n",
      "                 special = True           not_re : op_rel =      4.0 : 1.0\n",
      "                together = True           not_re : op_rel =      4.0 : 1.0\n",
      "                 transit = True           not_re : op_rel =      4.0 : 1.0\n",
      "                   using = True           not_re : op_rel =      4.0 : 1.0\n",
      "              vulnerable = True           not_re : op_rel =      4.0 : 1.0\n",
      "                  groups = True           not_re : op_rel =      4.0 : 1.0\n",
      "              complaints = True           op_rel : not_re =      3.8 : 1.0\n",
      "                   eight = True           op_rel : not_re =      3.8 : 1.0\n",
      "                    poor = True           op_rel : not_re =      3.8 : 1.0\n",
      "                     run = True           op_rel : not_re =      3.8 : 1.0\n",
      "                  saying = True           op_rel : not_re =      3.8 : 1.0\n",
      "                arrested = True           op_rel : not_re =      3.8 : 1.0\n",
      "                    held = True           not_re : op_rel =      3.6 : 1.0\n",
      "                  always = True           not_re : op_rel =      3.4 : 1.0\n",
      "                    data = True           not_re : op_rel =      3.4 : 1.0\n",
      "                  mental = True           not_re : op_rel =      3.4 : 1.0\n",
      "                  adding = True           op_rel : not_re =      3.3 : 1.0\n",
      "                 capital = True           op_rel : not_re =      3.3 : 1.0\n",
      "                domestic = True           op_rel : not_re =      3.3 : 1.0\n",
      "                 earlier = True           op_rel : not_re =      3.3 : 1.0\n",
      "             trafficking = False          not_re : op_rel =      3.2 : 1.0\n",
      "                     man = True           op_rel : not_re =      3.2 : 1.0\n",
      "                 various = True           op_rel : not_re =      3.2 : 1.0\n",
      "                arrested = False          not_re : op_rel =      3.1 : 1.0\n",
      "                Minister = True           not_re : op_rel =      3.1 : 1.0\n",
      "               available = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 billion = True           not_re : op_rel =      3.1 : 1.0\n",
      "                capacity = True           not_re : op_rel =      3.1 : 1.0\n",
      "                   claim = True           not_re : op_rel =      3.1 : 1.0\n",
      "                concerns = True           not_re : op_rel =      3.1 : 1.0\n",
      "              considered = True           not_re : op_rel =      3.1 : 1.0\n",
      "               continued = True           not_re : op_rel =      3.1 : 1.0\n",
      "                declared = True           not_re : op_rel =      3.1 : 1.0\n",
      "                  enough = True           not_re : op_rel =      3.1 : 1.0\n",
      "             environment = True           not_re : op_rel =      3.1 : 1.0\n",
      "                  facing = True           not_re : op_rel =      3.1 : 1.0\n",
      "                    full = True           not_re : op_rel =      3.1 : 1.0\n",
      "                   image = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 leaders = True           not_re : op_rel =      3.1 : 1.0\n",
      "                     low = True           not_re : op_rel =      3.1 : 1.0\n",
      "               migration = True           not_re : op_rel =      3.1 : 1.0\n",
      "                military = True           not_re : op_rel =      3.1 : 1.0\n",
      "                     old = True           not_re : op_rel =      3.1 : 1.0\n",
      "               political = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 records = True           not_re : op_rel =      3.1 : 1.0\n",
      "                    risk = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 sharing = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 society = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 stories = True           not_re : op_rel =      3.1 : 1.0\n",
      "                    task = True           not_re : op_rel =      3.1 : 1.0\n",
      "                   think = True           not_re : op_rel =      3.1 : 1.0\n",
      "                 alleged = True           op_rel : not_re =      2.9 : 1.0\n",
      "                 Tuesday = True           not_re : op_rel =      2.9 : 1.0\n",
      "                   major = True           not_re : op_rel =      2.9 : 1.0\n",
      "                   never = True           not_re : op_rel =      2.9 : 1.0\n",
      "                    open = True           not_re : op_rel =      2.9 : 1.0\n",
      "                    step = True           not_re : op_rel =      2.9 : 1.0\n",
      "                   works = True           not_re : op_rel =      2.9 : 1.0\n",
      "                    come = True           not_re : op_rel =      2.8 : 1.0\n",
      "                    high = True           not_re : op_rel =      2.8 : 1.0\n",
      "                  Agency = True           op_rel : not_re =      2.8 : 1.0\n",
      "                    June = True           op_rel : not_re =      2.8 : 1.0\n",
      "              everything = True           op_rel : not_re =      2.8 : 1.0\n",
      "                   faces = True           op_rel : not_re =      2.8 : 1.0\n",
      "                 instead = True           op_rel : not_re =      2.8 : 1.0\n",
      "            investigated = True           op_rel : not_re =      2.8 : 1.0\n",
      "MNB_classifier accuracy percent: 74.4186046511628\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(featuresets, test_size=0.4, random_state=4)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "\n",
    "print(\"Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test))*100)\n",
    "classifier.show_most_informative_features(100)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(train)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, test))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powered-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ngrams(document, ngram_features):\n",
    "    \"\"\"\n",
    "    Return a dictionary with boolean values indicating whether each ngram_feature is present in a document.\n",
    "    \"\"\"\n",
    "    ngrams = set(document)\n",
    "    found_ngrams = {}\n",
    "    for g in ngram_features:\n",
    "            found_ngrams[g] = (g in ngrams)\n",
    "    return found_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "taken-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles annotated only once: 1019 \n",
      "Number of articles annotated twice: 1119 \n",
      "Number of remaining conflicts between 1st and 2nd annotations: 198\n"
     ]
    }
   ],
   "source": [
    "# Get Training Set\n",
    "tagtog_path = os.getcwd() + \"\\\\tagtog_HT_News_12_21_21\"\n",
    "path_to_html = tagtog_path + r\"\\HT_News\\plain.html\\pool\"\n",
    "html_files = [pos_html for pos_html in os.listdir(path_to_html) if pos_html.endswith('.html')]\n",
    "\n",
    "prev_a_dict = get_article_dict(tagtog_path)\n",
    "update_doc_annotations(tagtog_path, prev_a_dict, print_incomplete_ann=False)\n",
    "update_doc_master_anns(tagtog_path, prev_a_dict)\n",
    "\n",
    "Article.merge_to_master()\n",
    "\n",
    "def check_master_agree(k, dict1):\n",
    "    try:\n",
    "        if dict1[k].master_ann.relevant != 'disagree':\n",
    "            return(k)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train_keys = [check_master_agree(k, prev_a_dict) for k in prev_a_dict.keys() \\\n",
    "              if check_master_agree(k, prev_a_dict)!=None]\n",
    "train_dict = {k:check_master_agree(v, prev_a_dict) for (k,v) in prev_a_dict.items()}\n",
    "#print(len(prev_a_dict))\n",
    "#print(len(train_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "emerging-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "389\n"
     ]
    }
   ],
   "source": [
    "keyword_unigrams = [\n",
    "    'slavery',\n",
    "    'trafficking',\n",
    "    'trafficked',\n",
    "    'humantrafficking',\n",
    "    'childtrafficking',\n",
    "    'antitrafficking',\n",
    "]\n",
    "\n",
    "keyword_bigrams = [\n",
    "('sex', 'racket'),\n",
    "('child', 'labour'),\n",
    "('child', 'labor'),\n",
    "('child', 'prostitute'),\n",
    "('teen', 'prostitute'),\n",
    "('teenage', 'prostitute'),\n",
    "]\n",
    "\n",
    "def get_ann_articles(dict1):\n",
    "    ann_articles = []\n",
    "\n",
    "    for k in dict1.keys():\n",
    "        try:\n",
    "            if dict1[k].master_ann.curator:\n",
    "                a_text = str(dict1[k].text + re.sub(r\"[-!?',;./]\", ' ', dict1[k].url)).translate(\n",
    "                str.maketrans('-', ' ', string.punctuation)).lower()\n",
    "                ann_articles.append((word_tokenize(a_text), \n",
    "                                     list(nltk.bigrams(a_text.split())),\n",
    "                                     dict1[k].ann1.relevant,\n",
    "                                     dict1[k].uid))\n",
    "        except:\n",
    "            pass\n",
    "    return ann_articles\n",
    "\n",
    "ann_articles = get_ann_articles(train_dict)\n",
    "\n",
    "train_test_keys = [check_master_agree(k, a_dict) for k in a_dict.keys() if check_master_agree(k, a_dict)!=None]\n",
    "test_dict = {k:v for (k,v) in a_dict.items() if (k in train_test_keys) and (k not in train_keys)}\n",
    "\n",
    "test_ann_articles = get_ann_articles(test_dict)\n",
    "\n",
    "\n",
    "print(len(ann_articles))\n",
    "print(len(test_ann_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "productive-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.16%\n",
      "False Negative Rate: 0.26%\n",
      "False Positive Rate: 66.58%\n",
      "True Positive Rate: 16.71%\n",
      "True Negative Rate: 16.45%\n",
      "\n",
      "Precision: 20.06%\n",
      "Recall: 98.48%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>UID</th>\n",
       "      <th>match</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>true_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.g3znPbHy4w5uaRK4GzP_a3FdiS-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.TxAsEN4lLuKIOLjfb_myJyEAvC-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.YNixBS7dNz0BBslac2KuDy2MES-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a0gW5H30w2xOQCppoOd6nDCWArQ4-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a1EjA3QRD.d4F.xM8jzL0zAxAB.W-jan22_remaining2....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_KXREyBtsUAgYQibndGV9f6HxUK-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_n1jHtG0DbynaxjOTiHH.na3cB0-Bibek_File2_filte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>a_NRV8U.eMMZw78YYtf4HY3Ya35C-Nov17_export50.cs...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>a_RcQvn9aolbVjt4VCTU4w5yha_S-jan22_remaining2....</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_VPkK0j0JKI4A7CxCFO89zNRi8a-2_15_22.csv_74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted  Actual                                                UID  \\\n",
       "0         True   False  a.g3znPbHy4w5uaRK4GzP_a3FdiS-Bibek_File2_filte...   \n",
       "1         True   False  a.TxAsEN4lLuKIOLjfb_myJyEAvC-Bibek_File2_filte...   \n",
       "2         True   False  a.YNixBS7dNz0BBslac2KuDy2MES-Bibek_File2_filte...   \n",
       "3         True   False  a0gW5H30w2xOQCppoOd6nDCWArQ4-Bibek_File2_filte...   \n",
       "4         True   False  a1EjA3QRD.d4F.xM8jzL0zAxAB.W-jan22_remaining2....   \n",
       "..         ...     ...                                                ...   \n",
       "384       True   False  a_KXREyBtsUAgYQibndGV9f6HxUK-Bibek_File2_filte...   \n",
       "385       True   False  a_n1jHtG0DbynaxjOTiHH.na3cB0-Bibek_File2_filte...   \n",
       "386       True    True  a_NRV8U.eMMZw78YYtf4HY3Ya35C-Nov17_export50.cs...   \n",
       "387       True    True  a_RcQvn9aolbVjt4VCTU4w5yha_S-jan22_remaining2....   \n",
       "388       True   False        a_VPkK0j0JKI4A7CxCFO89zNRi8a-2_15_22.csv_74   \n",
       "\n",
       "     match  false_negative  false_positive  true_positive  true_negative  \n",
       "0        0               0               1              0              0  \n",
       "1        0               0               1              0              0  \n",
       "2        0               0               1              0              0  \n",
       "3        0               0               1              0              0  \n",
       "4        0               0               1              0              0  \n",
       "..     ...             ...             ...            ...            ...  \n",
       "384      0               0               1              0              0  \n",
       "385      0               0               1              0              0  \n",
       "386      1               0               0              1              0  \n",
       "387      1               0               0              1              0  \n",
       "388      0               0               1              0              0  \n",
       "\n",
       "[389 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(find_ngrams(arti_w, keyword_unigrams), \n",
    "                find_ngrams(arti_bi, keyword_bigrams), cat, uid) \\\n",
    "               for (arti_w, arti_bi, cat, uid) in test_ann_articles]\n",
    "\n",
    "def get_rel_pred_accuracy(feat_sets):\n",
    "    rel_dict = []\n",
    "    for i in range(len(feat_sets)):\n",
    "        rel_dict.append(\n",
    "            {\n",
    "                'Predicted': True in feat_sets[i][0].values() or True in feat_sets[i][1].values(),\n",
    "                'Actual': feat_sets[i][2] == 'op_relevant',\n",
    "                'UID':  feat_sets[i][3]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    rel_df = pd.DataFrame(rel_dict)\n",
    "\n",
    "    rel_df['match'] = np.where(rel_df.Predicted == rel_df.Actual, 1, 0)\n",
    "    print(\"Accuracy: \" + \"{:.2%}\".format(sum(rel_df.match)/len(rel_df)))\n",
    "\n",
    "    rel_df['false_negative'] = np.where((rel_df.Predicted == 0) & (rel_df.Actual == 1), 1, 0)\n",
    "    FN = sum(rel_df.false_negative)/len(rel_df)\n",
    "    print('False Negative Rate: ' + str(\"{:.2%}\".format(FN)))\n",
    "\n",
    "    rel_df['false_positive'] = np.where((rel_df.Predicted == 1) & (rel_df.Actual == 0), 1, 0)\n",
    "    FP = sum(rel_df.false_positive)/len(rel_df)\n",
    "    print('False Positive Rate: ' + str(\"{:.2%}\".format(FP)))\n",
    "\n",
    "    rel_df['true_positive'] = np.where((rel_df.Predicted == 1) & (rel_df.Actual == 1), 1, 0)\n",
    "    TP = sum(rel_df.true_positive)/len(rel_df)\n",
    "    print('True Positive Rate: ' + str(\"{:.2%}\".format(TP)))\n",
    "\n",
    "    rel_df['true_negative'] = np.where((rel_df.Predicted == 0) & (rel_df.Actual == 0), 1, 0)\n",
    "    TN = sum(rel_df.true_negative)/len(rel_df)\n",
    "    print('True Negative Rate: ' + str(\"{:.2%}\".format(TN)))\n",
    "\n",
    "    print('\\nPrecision: ' + str(\"{:.2%}\".format(TP/(TP + FP))))\n",
    "    print('Recall: ' + str(\"{:.2%}\".format(TP/(TP + FN))))\n",
    "    return rel_df\n",
    "\n",
    "rel_df = get_rel_pred_accuracy(featuresets)\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recognized-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 503\n",
      "\n",
      "Accuracy: 28.63%\n",
      "False Negative Rate: 0.00%\n",
      "False Positive Rate: 71.37%\n",
      "True Positive Rate: 6.76%\n",
      "True Negative Rate: 21.87%\n",
      "\n",
      "Precision: 8.65%\n",
      "Recall: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "      <th>UID</th>\n",
       "      <th>match</th>\n",
       "      <th>false_negative</th>\n",
       "      <th>false_positive</th>\n",
       "      <th>true_positive</th>\n",
       "      <th>true_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a..nn9ofRVdv6uc3IXRsxrCL69CS-12_2_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.4jJuOgYmhR8ONnY0S58WPSNJ80-12_9_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.ENWjpr1JBeYW793OGqMRVTS.Um-12_9_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.q.REJp0pMHBTrhtca5n5Wj6t_y-12_9_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a.YZ0P.Wc56c6NJRPvIYFcRhOB6C-12_6_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_nvtl58I_kOi3qHcOMmuhW3xxa8-12_9_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>a_O9.pZoBZpgN8cXG71wTqteTfpS-1st_batch_wiki_re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_TlzpXaspTo93clUwqcM1UA3nWq-12_2_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_vhO3zBGWhiUm0481KyndPVOS9a-12_9_21export50.c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>a_YDtntyAXgK9YxaEpSB4GzpFEgi-1st_batch_wiki_re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted  Actual                                                UID  \\\n",
       "0         True   False  a..nn9ofRVdv6uc3IXRsxrCL69CS-12_2_21export50.c...   \n",
       "1         True   False  a.4jJuOgYmhR8ONnY0S58WPSNJ80-12_9_21export50.c...   \n",
       "2         True   False  a.ENWjpr1JBeYW793OGqMRVTS.Um-12_9_21export50.c...   \n",
       "3         True   False  a.q.REJp0pMHBTrhtca5n5Wj6t_y-12_9_21export50.c...   \n",
       "4         True   False  a.YZ0P.Wc56c6NJRPvIYFcRhOB6C-12_6_21export50.c...   \n",
       "..         ...     ...                                                ...   \n",
       "498       True   False  a_nvtl58I_kOi3qHcOMmuhW3xxa8-12_9_21export50.c...   \n",
       "499      False   False  a_O9.pZoBZpgN8cXG71wTqteTfpS-1st_batch_wiki_re...   \n",
       "500       True   False  a_TlzpXaspTo93clUwqcM1UA3nWq-12_2_21export50.c...   \n",
       "501       True   False  a_vhO3zBGWhiUm0481KyndPVOS9a-12_9_21export50.c...   \n",
       "502       True   False  a_YDtntyAXgK9YxaEpSB4GzpFEgi-1st_batch_wiki_re...   \n",
       "\n",
       "     match  false_negative  false_positive  true_positive  true_negative  \n",
       "0        0               0               1              0              0  \n",
       "1        0               0               1              0              0  \n",
       "2        0               0               1              0              0  \n",
       "3        0               0               1              0              0  \n",
       "4        0               0               1              0              0  \n",
       "..     ...             ...             ...            ...            ...  \n",
       "498      0               0               1              0              0  \n",
       "499      1               0               0              0              1  \n",
       "500      0               0               1              0              0  \n",
       "501      0               0               1              0              0  \n",
       "502      0               0               1              0              0  \n",
       "\n",
       "[503 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing approach on articles that have only been annotated once so far:\n",
    "test_ann_articles = []\n",
    "\n",
    "for k in a_dict.keys():\n",
    "    try:\n",
    "        if a_dict[k].master_ann == None:\n",
    "            a_text = str(a_dict[k].text + re.sub(r\"[-!?',;./]\", ' ', a_dict[k].url)).translate(\n",
    "                str.maketrans('-', ' ', string.punctuation)).lower()\n",
    "            test_ann_articles.append((word_tokenize(a_text), \n",
    "                                 list(nltk.bigrams(a_text.split())),\n",
    "                                 a_dict[k].ann1.relevant,\n",
    "                                 a_dict[k].uid))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Test set size: \" + str(len(test_ann_articles)) + \"\\n\")\n",
    "\n",
    "test_featuresets = [(find_ngrams(arti_w, keyword_unigrams), \n",
    "                find_ngrams(arti_bi, keyword_bigrams), cat, uid) \\\n",
    "               for (arti_w, arti_bi, cat, uid) in test_ann_articles]\n",
    "\n",
    "get_rel_pred_accuracy(test_featuresets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
